*** 2019 - 03 - 01星期五 ***
- 小轿车业务逻辑处理
- 选择目的地定位错误的bug
（3小时）
- ElasticSearch系列文章第一章：
http://uninote.com.cn/book/1079089832#960
（5小时）

*** 2019 - 03 - 02 星期六 ***
- 小轿车发版
- ElasticSearch系列文章第二章：
http://uninote.com.cn/book/1079089832#961
（2小时）
- ElasticSearch系列文章第三章：
http://uninote.com.cn/book/1079089832#962
（6小时）

*** 2019 - 03 - 05 星期二 ***
- ElasticSearch系列文章第四章：
http://uninote.com.cn/book/1079089832#977
（4小时）
- 优化文章ElasticSearch安装教程
http://uninote.com.cn/book/1079089832#565
（0.5小时）
- ElasticSearch系列文章第五章上半部分：
http://uninote.com.cn/book/1079089832#981
（3小时）

*** 2019 - 03 - 06 星期三 ***
- ElasticSearch系列文章第五章下半部分：
http://uninote.com.cn/book/1079089832#981
（2小时）
- ElasticSearch系列文章第六章：
http://uninote.com.cn/book/1079089832#1086
（2.30小时）
- ElasticSearch系列文章第七章：
http://uninote.com.cn/book/1079089832#1092
（1.30小时)
- Linux设置虚拟内存实战教程：
https://juejin.im/post/5c7fb7e9f265da2dcf62ad43
（2小时）

*** 2019 - 03 - 07 星期四 ***
1. 学习爬虫库requests,bs4
(4小时)
2. 分析爬虫方向以及分析爬取那些内容能够带实质效果
(1小时)
3. 学习爬虫核心Scrapy框架
(4小时)

*** 2019 - 03 - 08 星期五 ***
1. 学习爬虫核心Scrapy框架, spider,pipeline
(9小时)
2. 写基础掘金爬虫
(2小时)

*** 2019 - 03 - 09 星期六 ***
1. 写掘金文章爬虫
(8小时)

*** 2019 - 03 - 11 星期一 ***
1. 弄批量文章爬虫
(2小时)
2. 弄es数据可视化
(6小时)

*** 2019 - 03 - 12 星期二 ***
1. es数据可视化
（2小时）
1. 写用户爬虫
（4小时）
3. windows 搭建爬虫环境
（3小时）


*** 2019 - 03 - 13 星期三 ***
1. 监控数据爬取, 并持续优化爬虫
(全天)
2. 写爬虫文章总结
（4小时）
https://juejin.im/post/5c88bb19f265da2d96184df3#heading-6
3. 优化爬虫，现在的爬虫只能看到现有的网站上url连接到的数据，无法获取所有数据，所以这次优化是通过分析掘金的文章所处位置都在所有标签里面，通过分析标签然后一步一步的获取所有文章和用户
（2小时）

*** 2019 - 03 - 14 星期四 ***
1. 优化爬虫，现在的爬虫只能看到现有的网站上url连接到的数据，无法获取所有数据，所以这次优化是通过分析掘金的文章所处位置都在所有标签里面，通过分析标签然后一步一步的获取所有文章和用户
（6小时）
2. 现在是可以获取所有的文章了，但是还有一个问题就是掘金做了防止高频率访问，做了ip限制httpcode为429，导致有些文章无法拉去下来，这里需要找一下解决办法，现在找到的解决办法就是通过代理ip，但是现在能找到的代理ip好像都有问题，已经都失效了，所以这个问题还在持续解决中。

*** 2019 - 03 - 15 星期五 ***
1. 将所有标签预加载到对列里面，分析队列的数据，分别爬取文章和用户数据。
（3 小时）
2. 弄了一下docker，打算将现有的环境弄到docker里面，因为每次这个爬虫迁移环境的体积太大，很多无用功，十分耗时，所以研究了一下怎么弄到docker里面去，现在碰到的问题是docker的两个容器间通信好像还有点问题，明天继续研究看看是怎么回事。
(6 小时)

*** 2019 - 03 - 16 星期六 ***
1. 解决容器间通信问题，原因是容器间通过ip访问时，必须关闭防火墙或者暴露端口（昨天也弄过这个步骤，但是不知道为什么没生效）,但是这里必须要打开防火墙，可以通过暴露端口的形式访问。
(2 小时)
2. 由于每次在docker-composer里面获取的内容ip都必须写成内网的IP，所以这样也不能解决办法，需要找到一个不用设置就可以直接通过一个ip和端口就可以访问其他容器内容的办法
(7 小时)

*** 2019 - 03 - 18 星期一 ***
1. 解决3月16的容器固定IP问题，可以使用docker创建一个网络，然后让这写容器都加入到这个网络里面，然后将他们每个容器的网络命名为一个别名，然后就可以访问了。
详细解决方案在这儿，http://uninote.com.cn/book/1079089832#1125
(5 小时)
2. 通过docker-compose已经可以直接启动redis和elasticsearch还有kibana了，并且他们之间可以通信，现在的问题是爬虫项目不能直接在docker-compose里面启动，必须要手动启动才行，手动启动也是可以和其他组件进行交互了，可以爬到内容，现在要解决的就是如何让爬虫项目一起启动，不让他停止
(4 小时)

*** 2019 - 03 - 19 星期二 ***
1. 分布式爬虫scrapy-redis的学习及使用
(5 小时)
2. 检查爬虫数据丢失原因
(2 小时)
	1. 在爬取的数量大于文章总数量时，不会返回数据，必须降低页数
	2. 链接过滤不能采用https的方式

3. 重构掘金爬虫
(2 小时)

*** 2019 - 03 -20 星期三 ***
1. 文章添加标签
2. 文章会丢失信息处理，有时候发起的请求会请求失败，这种请求要重新发起请求
3. 记录爬虫日志
	1. 现在的爬虫日志是记录到容器里面的
4. 请求失败放入队列继续请求
	1. 有的请求会失败，是掘金那边服务器抛出的错误，爬虫里面遇到这种直接将这个请求放入队列，然后等下次继续跑
5. 跑过的标签和url放入一个新的列表里面
6. redis获取失败时，必须让redis重新连接
7. 429的数据和请求失败的数据全部放入request队列里面重新发起请求
	1. 有的请求坑会被掘金的防爬机制视为恶意请求，这种情况需要将请求重新放入对垒继续执行
8. windows 安装docker
	1. windows 现在只有win10才能安装docker，win7安装docker必须要找到相应的版本才行，这个找了很久才找到win7的版本
9. 爬虫在启动的时候必须要redis和elasticsearch都启动起来了才能运行

*** 2019 - 03 -21 星期四 ***
1. docker使用文章
http://uninote.com.cn/book/1079089832#1144
2. 爬虫部署信息
http://uninote.com.cn/book/1079089832#1142
3. 掘金爬虫整理
http://uninote.com.cn/book/1079089832#1145

*** 2019 - 03 -22 星期五 ***
1. es查询速度慢检查
2. es去掉拼音分词
3. es环境部署

*** 2019 - 03 -23 星期六 ***
1. es查询速度慢排查
	1. 本地访问es服务器速度快，但是其他服务器访问es服务器慢，所以就以为不是网络的原因，以为是其他服务器接受数据包速度太慢，然后插叙了一下tcp的数据包，发现发送的数据包要比本地的的多，然后是因为数据发送丢失，然后tcp又重新发送数据


*** 2019 - 03 -25 星期一 ***
1. elasticsearch部署文档
http://uninote.com.cn/book/1079089832#1152
2. 处理es拼音高亮问题,
处理方式是修改mapping的分词器，达到高亮。没有采用多音字，多音字会导致结果不理想
3. 黑客专用系统之Kali Linux安装教程 http://uninote.com.cn/book/1079089832#1150

*** 2019 - 03 -26 星期二 ***
1. 技术栈 （2小时）
2. 处理es拼音搜索，当搜索汉字时不能使用拼音（3小时）
3. Kali Linux之信息采集
http://uninote.com.cn/book/1079089832#1155 （2小时）
4. 看网络是怎样连接的之第一章（浏览器生成消息-探索浏览器内部）
第一章总结：
	1. 浏览器输入一个网址的时候，比如是http://www.baidu.com
	2. 浏览器会解析这个网址，先分析这个网址是什么协议，然后根据网址的协议来生成相应的请求数据
	3. 分析结果得到这是http协议，那么就会生成一个http协议的报文
	4. 得到报文过后需要知道www.baidu.com 的ip地址，由于浏览器并不知道直接发起外部网络请求，所有只有委托给操作系统
	5. 操作系统收到了浏览器的查询ip地址的委托，操作系统就调用了socket库里面的域名解析器，让域名解析器向tcp/ip里面的dns服务器发起查询，查询的请求内容是 {"域名' = "www.baidu.com", Class = IN , 类型 = A}
	6. 由于tcp/ip里面设置的dns服务器基本上不可能包含你当前的这个域名，然后dns服务器就分析这个域名首先去询问根级域名是否有这个地址（域名除了顶级域名还有一个根级域名，使用.表示，它在顶级域名后面），这个根级域名就是. ，根级域名会返回一个信息，信息里面会说明是否包含这个地址，如果不包含那么根级域名也会返回一个ip地址，这个ip地址就是顶级域名（根级域名是通过传输过来的域名分析出来的）com，然后这个dns服务器带上消息去com所在的服务器询问是否有这个地址，com 回答的是还是没有，但是又返回了一个baidu.com 这个域名的ip，让dns服务器去找它要，然后dns服务器就又去找baidu.com 这个IP所对应的ip查找是否有域名，baidu.com 的回答是有，并且给了一个www.baidu.com 的ip地址。
	7. 客户端拿到了www.baidu.com 的ip地址过后，然后就通知浏览器已经拿到了ip地址
	8. 浏览器收到通知过后就开始发送消息，但是想要发送消息又必须委托操作系统，因为浏览器并不具备发起网络请求的功能，所以只有找到操作系统的协议栈来发送请求
	9. 操作系统接收到浏览器想要发送请求，操作系统就开始和域名所对应的ip发起通信，但是想要发起通讯就要建立一个数据通道，不然别人就来污染这个数据了，所有客户端和服务器都会建立一个套接字，两端的套接字连接上就代表数据通道连接上了，但是浏览器肯定不只发送一次请求，但是又会建立很多的套接字，所以这个时候需要区分套接字的唯一性，所以就为每个套接字打上了标识符，所以操作系统在发送数据的时候就需要发送一个标识符到服务器端，服务器接受到数据，就将相应的数据返回给客户端，并且将标识符返回，然后在断开和客服端的连接，客户端收到了数据过后需要使用标识符来取数据，取到数据就给浏览器，这就是整个流程

*** 2019 - 03 -27 星期三 ***
计划
在阅读一遍第一章，并且将第一章（浏览器生成消息-探索浏览器内部）写成文章

浏览器生成消息-探索浏览器内部 http://uninote.com.cn/book/1079089832#1170 (6小时)
阅读第二章用电信号传输 TCP/IP 数据（4小时）


*** 2019 - 03 -28 星期四***
- 处理es不返回content字段，这个字段是没有用的，所以就没有返回到客户端。
（0.1小时）

- 设置es数据安全访问，解决方式是采用内网访问，数据操作通过接口，如果想直接操作数据可以通过kibana来操作，但是必须为kibana做nginx密码认证，这样就可以解决别人可以操作我们的数据的问题了。

- 在虚拟机里面先搭建上诉环境并测试

- 文章
http://uninote.com.cn/book/1079089832#1178

- 查找将pdf转换为markdown的方式
网上的工具基本上没法转，文件太大，只有将它拆分开转，转成html然后在转md，比直接写还要麻烦些。
已经试过的：
https://github.com/fagbokforlaget/pdftohtmljs
https://github.com/miohtama/pdf-to-html
https://github.com/johnlinp/pdf-to-markdown
windows上面的转转大师
windows 上面使用迅捷可以转换



*** 2019 - 03 -29 星期五***
- 将第一章以书籍介绍的形式展示出来
- 将pdf上传到服务器，使用服务器的方式访问。
- 看网络是怎样连接的第二章





